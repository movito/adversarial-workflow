# v0.6.0 Roadmap: Plugin Architecture

**Target Release**: Q1 2026
**Theme**: Extensibility & Domain-Specific Evaluation
**Status**: Planning

## Executive Summary

Version 0.6.0 introduces a plugin architecture that allows projects to define custom evaluators without modifying the installed package. This is the foundation for domain-specific AI evaluation (knowledge work, legal review, security audits, etc.).

## Motivation

Based on real-world implementation experience from the ombruk-idrettsbygg project:

> "When implementing Athena (a knowledge evaluator using Gemini 2.5 Pro), we couldn't add a new evaluator without modifying the installed package. This works but breaks on upgrade."

**The problem**: Teams need domain-specific evaluators with different models and prompts, but current options are:
- Modify installed package (breaks on `pip upgrade`)
- Fork the package (maintenance burden)
- Wait for upstream release (blocks progress)

**The solution**: Local evaluator definitions in `.adversarial/evaluators/*.yml`

## Flagship Feature: Local Evaluator Definitions

### What It Enables

```yaml
# .adversarial/evaluators/athena.yml
name: athena
description: Knowledge evaluation using Gemini 2.5 Pro
model: gemini-2.5-pro
api_key_env: GEMINI_API_KEY
output_suffix: KNOWLEDGE-EVALUATION
prompt: |
  You are Athena, a knowledge evaluation specialist.
  You evaluate research, documentation, and knowledge bases...
aliases:
  - knowledge
  - research
```

Then use it:
```bash
adversarial athena docs/research-findings.md
```

### Why This Matters

| Benefit | Description |
|---------|-------------|
| **Upgrade-safe** | Local definitions survive `pip upgrade` |
| **Version controlled** | `.adversarial/evaluators/` commits to git |
| **Model diversity** | Use Gemini for knowledge, Claude for code, GPT-4o for plans |
| **Domain-specific** | Legal, medical, security, compliance evaluators |
| **Shareable** | Copy evaluator definitions between projects |

### Real Validation

From ombruk-idrettsbygg's Athena implementation:

```
Task: OIB-0005 (Knowledge base documentation)

GPT-4o (evaluate):
  Verdict: NEEDS_REVISION
  Focus: "Missing error handling", "edge cases in script"
  Relevance: LOW (code concepts on research task)

Gemini 2.5 Pro (athena):
  Verdict: APPROVED
  Focus: Source validity, maintainability, audience fit
  Relevance: HIGH (knowledge-appropriate feedback)
```

## Release Contents

### Core Features (Must Have)

| Feature | Task | Status |
|---------|------|--------|
| Evaluator Discovery | ADV-0013 | Todo |
| YAML Definition Parser | ADV-0013 | Todo |
| Generic Evaluator Runner | ADV-0013 | Todo |
| CLI Dynamic Registration | ADV-0013 | Todo |
| `list-evaluators` Command | ADV-0013 | Todo |

### Quality (Must Have)

| Feature | Description |
|---------|-------------|
| Unit Tests | Config parsing, discovery, runner |
| Integration Tests | CLI commands, backwards compatibility |
| Documentation | Evaluator definition format, examples |

### Nice to Have (May Defer)

| Feature | Description | Decision |
|---------|-------------|----------|
| Prompt Templating | `{{file_name}}` variables | Defer to v0.7 |
| Schema Validation | JSON Schema for YAML | Defer to v0.7 |
| Evaluator Inheritance | Extend other evaluators | Defer to v0.7 |

## Technical Architecture

### New Module Structure

```
adversarial_workflow/
├── evaluators/           # NEW
│   ├── __init__.py
│   ├── config.py         # EvaluatorConfig dataclass
│   ├── discovery.py      # Discover built-in + local
│   ├── runner.py         # Generic evaluator execution
│   └── prompts.py        # Built-in evaluator prompts
├── cli.py                # Simplified, delegates to evaluators
└── ...
```

### Key Design Decisions

1. **Local overrides built-in**: A local `evaluate.yml` can customize the built-in evaluator
2. **Graceful degradation**: Invalid YAML files are skipped with warnings
3. **No new dependencies**: Uses existing `pyyaml`
4. **Fast discovery**: <100ms added to CLI startup

## Migration & Compatibility

### Breaking Changes

None. This release is fully backwards compatible.

### Deprecations

None planned.

### Migration Path

Existing projects work unchanged. Plugin system is opt-in:
1. Create `.adversarial/evaluators/` directory
2. Add YAML evaluator definitions
3. Use new evaluators immediately

## Dependencies

### Upstream

- Python 3.10+ (unchanged from v0.5)
- aider-chat (unchanged)
- pyyaml (already a dependency)

### Related Tasks

| Task | Description | Status |
|------|-------------|--------|
| ADV-0013 | Plugin Architecture Phase 1 | Todo |

## Timeline

| Milestone | Target | Description |
|-----------|--------|-------------|
| Design Complete | Done | Architecture assessment, task spec |
| Implementation Start | TBD | Begin ADV-0013 |
| Alpha | TBD | Feature complete, testing |
| Release Candidate | TBD | Documentation complete |
| Release | Q1 2026 | PyPI publish |

## Future Versions

### v0.7.0: Plugin Architecture Phase 2

- Convert built-in evaluators to YAML format internally
- Prompt templating with variables
- JSON Schema validation for evaluator definitions

### v0.8.0: Plugin Architecture Phase 3

- `adversarial install-evaluator <name>` from community registry
- Version management for evaluators
- Dependency handling

## References

- **Proposal**: `ombruk/docs/proposals/ADVERSARIAL-WORKFLOW-PLUGIN-ARCHITECTURE.md`
- **Task Spec**: `delegation/tasks/2-todo/ADV-0013-plugin-architecture-phase1.md`
- **Branch**: `feature/plugin-architecture`

## Success Metrics

| Metric | Target |
|--------|--------|
| All existing tests pass | 100% |
| New test coverage | >80% for evaluators module |
| Documentation | Complete evaluator definition guide |
| Performance | <100ms discovery overhead |

---

**Prepared by**: Planner Agent
**Date**: 2026-01-14
**Version**: Draft 1.0
